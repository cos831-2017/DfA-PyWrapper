{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo Spark.\n",
    "Exemplo de dataflow especificado no SPARK instrumentado utilizando o wrapper desenvolvido em python.\n",
    "\n",
    "## Composição:\n",
    "Duas atividades de mapeamento que apenas recebem dados de entrada e os jogam na saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "import findspark\n",
    "findspark.init('/opt/spark-2.2.0-bin-hadoop2.7')\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import DataFrameWriter, Row\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from PDE import PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o spark\n",
    "conf = ( SparkConf()\n",
    "         .setMaster(\"local[*]\")\n",
    "         .setAppName('pyspark')\n",
    "        )\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "base_url = \"http://localhost:22000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapTransformation(object):\n",
    "    def __init__(self,rdd):\n",
    "        self._rdd = rdd\n",
    "    \n",
    "    def do_nothing(self):\n",
    "        return self._rdd.map(lambda s: s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configura proveniência prospectiva.\n",
    "\n",
    "#Define o dataflow\n",
    "p1 = Popen(['java', '-jar', 'PDG-1.0.jar','-dataflow','-tag','spark-example'])\n",
    "p1.wait()\n",
    "# Define as transformações.\n",
    "p2 = Popen(['java', '-jar', 'PDG-1.0.jar','-transformation','-dataflow',\n",
    "       'spark-example','-tag','dt1'])\n",
    "p2.wait()\n",
    "p3= Popen(['java', '-jar', 'PDG-1.0.jar','-transformation','-dataflow',\n",
    "       'spark-example','-tag','dt2'])\n",
    "p3.wait()\n",
    "\n",
    "# Define os programas\n",
    "p4 = Popen(['java', '-jar', 'PDG-1.0.jar','-program',\n",
    "            '-dataflow','spark-example','-transformation',\n",
    "            'dt1','-name','testando','-filepath','path'])\n",
    "p4.wait()\n",
    "\n",
    "# Define os Datasets.\n",
    "p5 = Popen(['java', '-jar', 'PDG-1.0.jar','-set','-dataflow',\n",
    "       'spark-example','-transformation','dt1','-tag',\n",
    "       'ds1','-type','output'])\n",
    "p5.wait()\n",
    "\n",
    "p6 = Popen(['java', '-jar', 'PDG-1.0.jar','-set','-dataflow',\n",
    "       'spark-example','-transformation','dt2','-tag',\n",
    "       'ds2','-type','input'])\n",
    "p6.wait()\n",
    "\n",
    "p7 = Popen(['java', '-jar', 'PDG-1.0.jar','-set','-dataflow',\n",
    "       'spark-example','-transformation','dt2','-tag',\n",
    "       'ds3','-type','output'])\n",
    "p7.wait()\n",
    "\n",
    "# Define os extratores.\n",
    "p8 = Popen(['java', '-jar', 'PDG-1.0.jar','-extractor',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "            'dt1','-set','ds1','-tag','ext1',\n",
    "            '-algorithm','extraction','csv'])\n",
    "p8.wait()\n",
    "\n",
    "p9 = Popen(['java', '-jar', 'PDG-1.0.jar','-extractor',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "        'dt2','-set','ds2','-tag','ext2','-algorithm',\n",
    "        'extraction','csv'])\n",
    "p9.wait()\n",
    "\n",
    "p10 = Popen(['java', '-jar', 'PDG-1.0.jar','-extractor',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "        'dt2','-set','ds3','-tag','ext3','-algorithm',\n",
    "        'extraction','csv'])\n",
    "p10.wait()\n",
    "\n",
    "# Define os atributos.\n",
    "p11 = Popen(['java', '-jar', 'PDG-1.0.jar','-attribute',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "        'dt1','-set','ds1','-name','att1','-type','text',\n",
    "        '-extractor','ext1'])\n",
    "p11.wait()\n",
    "\n",
    "p12 = Popen(['java', '-jar', 'PDG-1.0.jar','-attribute',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "        'dt1','-set','ds1','-name','att2','-type',\n",
    "        'numeric','-extractor','ext1'])\n",
    "p12.wait()\n",
    "\n",
    "p13 = Popen(['java', '-jar', 'PDG-1.0.jar','-attribute','-dataflow',\n",
    "       'spark-example','-transformation','dt2',\n",
    "        '-set','ds2','-name','att3','-type',\n",
    "        'text','-extractor','ext2'])\n",
    "p13.wait()\n",
    "\n",
    "p14 = Popen(['java', '-jar', 'PDG-1.0.jar','-attribute',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "        'dt2','-set','ds2','-name','att4','-type',\n",
    "        'numeric','-extractor','ext2'])\n",
    "p14.wait()\n",
    "\n",
    "p15 = Popen(['java', '-jar', 'PDG-1.0.jar','-attribute',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "        'dt2','-set','ds3','-name','att5','-type','text',\n",
    "        '-extractor','ext3'])\n",
    "p15.wait()\n",
    "\n",
    "p16 = Popen(['java', '-jar', 'PDG-1.0.jar','-attribute',\n",
    "        '-dataflow','spark-example','-transformation',\n",
    "        'dt2','-set','ds3','-name','att6','-type',\n",
    "        'numeric','-extractor','ext3'])\n",
    "p16.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configura proveniência retrospectiva e execução do dataflow.\n",
    "\n",
    "# Configura Task 1\n",
    "p17 = Popen(['java', '-jar', 'PDG-1.0.jar','-task',\n",
    "            '-dataflow','spark-example',\n",
    "            'transformation','dt1',\n",
    "             '-id','t1','-subid',\n",
    "             '1','-resouce','resource',\n",
    "             '-workspace','workspace',\n",
    "             '-status','running'])\n",
    "p17.wait()\n",
    "\n",
    "# Cria performances para t1.\n",
    "p18 = Popen(['java', '-jar', 'PDG-1.0.jar','-performance',\n",
    "             '-dataflow','spark-example',\n",
    "             '-transformation','dt1','-task','t1',\n",
    "             '-starttime','-description',\n",
    "             'pf1','-computation','description'])\n",
    "p18.wait()\n",
    "\n",
    "# Cria input.\n",
    "rdd = sc.parallelize([Row(att1=str(uuid.uuid4()),\n",
    "        att2=randint(0,100)) for x in range(5)])\n",
    "\n",
    "m1 = MapTransformation(rdd)\n",
    "\n",
    "r2 = m1.do_nothing()\n",
    "\n",
    "# Escreve em csv.\n",
    "df1 = sqlContext.createDataFrame(r2)\n",
    "\n",
    "# Arquivo de saída de t1.\n",
    "p19 = Popen(['java', '-jar', 'PDG-1.0.jar','-file','-dataflow',\n",
    "             'spark-example','transformation','dt1',\n",
    "             '-id','t1','-name','dt1-output.csv',\n",
    "             '-path','.'])\n",
    "p19.wait()\n",
    "\n",
    "df1.coalesce(1).write.save(path='dt1-output.csv', format='csv',\n",
    "                           mode='append', sep=',')\n",
    "\n",
    "# Adiciona informações à tarefa 1.\n",
    "p20 = Popen(['java', '-jar', 'PDG-1.0.jar','-performance',\n",
    "             '-dataflow','spark-example',\n",
    "             '-transformation','dt1','-task','t1',\n",
    "             '-endtime','-description','pf1',\n",
    "             '-computation','description'])\n",
    "p20.wait()\n",
    "\n",
    "# Modifica o estado da tarefa 1 para finished.\n",
    "p21 = Popen(['java', '-jar', 'PDG-1.0.jar','-task',\n",
    "             '-dataflow','spark-example','transformation',\n",
    "             'dt1','-id','t1','-resouce','resource',\n",
    "             '-workspace','workspace','-status',\n",
    "             'finished'])\n",
    "p21.wait()\n",
    "# Lê do csv.\n",
    "\n",
    "# Configura Task 2.\n",
    "p22 = Popen(['java', '-jar', 'PDG-1.0.jar','-task','-dataflow',\n",
    "             'spark-example','transformation','dt2',\n",
    "             '-id','t2','-resouce','resource','-workspace'\n",
    "             ,'workspace','-status','running'])\n",
    "p22.wait()\n",
    "\n",
    "# Adiciona arquivo de entrada à t2.\n",
    "p23 = Popen(['java', '-jar', 'PDG-1.0.jar','-file',\n",
    "             '-dataflow','spark-example',\n",
    "             'transformation','dt2','-id','t2',\n",
    "             '-name','dt1-output.csv','-path','.'])\n",
    "p23.wait()\n",
    "\n",
    "# Criar arquivo de performance para t2.\n",
    "p24 = Popen(['java', '-jar', 'PDG-1.0.jar','-performance',\n",
    "             '-dataflow','spark-example',\n",
    "             '-transformation','dt2','-task','t2',\n",
    "             '-starttime','-description',\n",
    "             'pf2','-computation','description'])\n",
    "p24.wait()\n",
    "\n",
    "df2 = (sqlContext.read\n",
    "         .format(\"com.databricks.spark.csv\")\n",
    "         .option(\"header\", \"false\")\n",
    "         .load('dt1-output.csv'))\n",
    "\n",
    "# Carrega informações na task2.\n",
    "m2 = MapTransformation(df2.rdd)\n",
    "\n",
    "r3 = m2.do_nothing()\n",
    "\n",
    "# Adiciona informações à tarefa 2.\n",
    "p25 = Popen(['java', '-jar', 'PDG-1.0.jar','-performance',\n",
    "             '-dataflow','spark-example',\n",
    "             '-transformation','dt2','-task','t2',\n",
    "             '-endtime','-description','pf2',\n",
    "             '-computation','description'])\n",
    "p25.wait()\n",
    "\n",
    "# Modifica o estado da tarefa 2 para finished.\n",
    "p26 = Popen(['java', '-jar', 'PDG-1.0.jar',\n",
    "             '-task','-dataflow','spark-example'\n",
    "             ,'transformation','dt2','-id',\n",
    "             't2','-resouce','resource','-workspace'\n",
    "             ,'workspace','-status','finished'])\n",
    "p26.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
