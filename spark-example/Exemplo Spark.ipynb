{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo Spark.\n",
    "Exemplo de dataflow especificado no SPARK instrumentado utilizando o wrapper desenvolvido em python.\n",
    "\n",
    "## Composição:\n",
    "Duas atividades de mapeamento que apenas recebem dados de entrada e os jogam na saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "import findspark\n",
    "findspark.init('/opt/spark-2.2.0-bin-hadoop2.7')\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import DataFrameWriter, Row\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from PDE import PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o spark\n",
    "conf = ( SparkConf()\n",
    "         .setMaster(\"local[*]\")\n",
    "         .setAppName('pyspark')\n",
    "        )\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "base_url = \"http://localhost:22000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MapTransformation(object):\n",
    "    def __init__(self,rdd):\n",
    "        self._rdd = rdd\n",
    "    \n",
    "    def do_nothing(self):\n",
    "        return self._rdd.map(lambda s: s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura proveniência prospectiva.\n",
    "\n",
    "# Define o dataflow.\n",
    "dataflow = PDE.dataflow(\"spark-example\")\n",
    "\n",
    "# Define as transformações.\n",
    "dt1 = PDE.transformation(\"dt1\")\n",
    "dt2 = PDE.transformation(\"dt2\")\n",
    "\n",
    "# Define os atributos.\n",
    "att1 = PDE.attribute(\"att1\", PDE.attribute_type().TEXT.value)\n",
    "att2 = PDE.attribute(\"att2\", PDE.attribute_type().NUMERIC.value)\n",
    "att3 = PDE.attribute(\"att3\", PDE.attribute_type().TEXT.value)\n",
    "att4 = PDE.attribute(\"att4\", PDE.attribute_type().NUMERIC.value)\n",
    "att5 = PDE.attribute(\"att5\", PDE.attribute_type().TEXT.value)\n",
    "att6 = PDE.attribute(\"att6\", PDE.attribute_type().NUMERIC.value)\n",
    "\n",
    "\n",
    "# Define os extratores.\n",
    "ext1 = PDE.extractor(\"ext1\", PDE.extractor_cartridge().EXTRACTION.value, PDE.extractor_extension().CSV.value)\n",
    "ext2 = PDE.extractor(\"ext2\", PDE.extractor_cartridge().EXTRACTION.value, PDE.extractor_extension().CSV.value)\n",
    "ext3 = PDE.extractor(\"ext3\", PDE.extractor_cartridge().EXTRACTION.value, PDE.extractor_extension().CSV.value)\n",
    "\n",
    "# Adiciona os extratores aos atributos.\n",
    "att1._extractor = ext1._tag\n",
    "att2._extractor = ext1._tag\n",
    "att3._extractor = ext2._tag\n",
    "att4._extractor = ext2._tag\n",
    "att5._extractor = ext3._tag\n",
    "att6._extractor = ext3._tag\n",
    "\n",
    "# Define os Datasets.\n",
    "ds1 = PDE.set(\"ds1\", PDE.set_type().OUTPUT.value)\n",
    "ds2 = PDE.set(\"ds2\", PDE.set_type().INPUT.value)\n",
    "ds3 = PDE.set(\"ds3\", PDE.set_type().OUTPUT.value)\n",
    "\n",
    "# Adiciona os atributos aos datasets.\n",
    "ds1._attributes = [att1.get_json(),att2.get_json()]\n",
    "ds2._attributes = [att3.get_json(),att4.get_json()]\n",
    "ds3._attributes = [att5.get_json(),att6.get_json()]\n",
    "\n",
    "program  = PDE.program(\"testando\",\"path\")\n",
    "\n",
    "# Adiciona os extratores aos datasets.\n",
    "ds1._extractors = [ext1.get_json()]\n",
    "ds2._extractors = [ext2.get_json()]\n",
    "ds3._extractors = [ext3.get_json()]\n",
    "\n",
    "# Adiciona os datasets às transformações.\n",
    "dt1._sets = [ds1.get_json()]\n",
    "dt2._sets = [ds2.get_json(),ds3.get_json()]\n",
    "\n",
    "dt1._programs = [program.get_json()]\n",
    "# Adiciona as transformações ao dataflow\n",
    "dataflow._transformations = [dt1.get_json(),dt2.get_json()]\n",
    "\n",
    "# Realiza post na api restful do dfa para inserir o dataflow.\n",
    "PDE.ingest_dataflow_json(base_url,dataflow.get_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configura proveniência retrospectiva.\n",
    "t1 = PDE.task(\"t1\",dataflow._tag, dt1._tag, \"resource\", \"workspace\",PDE.status_type().READY.value, \"1\")\n",
    "t2 = PDE.task(\"t2\",dataflow._tag, dt2._tag, \"resource\", \"workspace\",PDE.status_type().READY.value, \"2\")\n",
    "\n",
    "# Cria performances para t1.\n",
    "pf1 = PDE.performance(\"pf1\",PDE.method_type().COMPUTATION.value,\"ti\" ,t1._id)\n",
    "\n",
    "# Inicializa a task1\n",
    "t1._status = PDE.status_type().RUNNING.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria input.\n",
    "rdd = sc.parallelize([Row(att1=str(uuid.uuid4()),att2=randint(0,100)) for x in range(5)])\n",
    "\n",
    "m1 = MapTransformation(rdd)\n",
    "\n",
    "r2 = m1.do_nothing()\n",
    "\n",
    "# Escreve em csv.\n",
    "df1 = sqlContext.createDataFrame(r2)\n",
    "\n",
    "# Arquivo de saída de t1.\n",
    "f1 = PDE.file(\"dt1-output.csv\", \".\")\n",
    "\n",
    "df1.coalesce(1).write.save(path=f1._tag, format='csv', mode='append', sep=',')\n",
    "\n",
    "# Adiciona informações à tarefa 1.\n",
    "pf1._end_time = \"tf\"\n",
    "t1._performances.append(pf1.get_json())\n",
    "t1._files.append(f1.get_json())\n",
    "t1._status = PDE.status_type().FINISHED.value\n",
    "\n",
    "# Armazena t1 na base.\n",
    "PDE.ingest_task_json(base_url,t1.get_json())\n",
    "\n",
    "# Lê do csv.\n",
    "\n",
    "# Adiciona arquivo de entrada à t2.\n",
    "t2._files.append(f1.get_json())\n",
    "# Criar arquivo de performance para t2.\n",
    "pf2 = PDE.performance(\"pf2\",PDE.method_type().COMPUTATION.value,\"ti\" ,t2._id)\n",
    "\n",
    "df2 = (sqlContext.read\n",
    "         .format(\"com.databricks.spark.csv\")\n",
    "         .option(\"header\", \"false\")\n",
    "         .load(f1._tag))\n",
    "\n",
    "# Carrega informações na task2.\n",
    "m2 = MapTransformation(df2.rdd)\n",
    "\n",
    "r3 = m2.do_nothing()\n",
    "\n",
    "# Adiciona informações à tarefa 2.\n",
    "pf2._end_time = \"tf\"\n",
    "t2._performances.append(pf1.get_json())\n",
    "t2._status = PDE.status_type().FINISHED.value\n",
    "\n",
    "# Armazena t2 na base.\n",
    "PDE.ingest_task_json(base_url,t2.get_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
